{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "from polyglot.text import Text\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class HTMLParser:\n",
    "    def __init__(self, main_url=''):\n",
    "        self.urls = []\n",
    "        self.texts = []\n",
    "        self.main_url = main_url\n",
    "        if urlparse(main_url).scheme == '':\n",
    "            self.main_url = 'https://' + self.main_url\n",
    "        print('initialize HTMLParser with \"{}\"'.format(self.main_url))\n",
    "\n",
    "    def _traverse(self, from_url=None):\n",
    "        if from_url is None:\n",
    "            from_url = self.main_url\n",
    "\n",
    "        links = BeautifulSoup(urllib.request.urlopen(from_url)).find_all('a')\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href.startswith('#'):\n",
    "                href = '/' + href\n",
    "            url = urljoin(from_url, href)\n",
    "            url = urlparse(url).geturl()\n",
    "            is_external = url.startswith('http') and not url.startswith(self.main_url)\n",
    "            if not is_external and not url in self.urls:\n",
    "                yield url\n",
    "                self._traverse(url)\n",
    "\n",
    "    \n",
    "    def _create_filename(self, url, i):\n",
    "        url_str = url.replace('https://', '').replace('#', '').replace('.', '_').replace('/', '_').replace('-', '_')\n",
    "        if i > 9999:\n",
    "            print('WARNING: paragraph list is too long')\n",
    "        return '{}_{:05d}'.format(url_str, i)\n",
    "    \n",
    "    \n",
    "    def _save_text(self, text, path_to_save):\n",
    "        with open(path_to_save, 'wb') as file:\n",
    "            pickle.dump(text, file)\n",
    "        #print('save texts to \\\"{}\\\"'.format(path_to_save))\n",
    "        \n",
    "    \n",
    "    def parse(self, path_to_save='texts'):\n",
    "        try:\n",
    "            os.mkdir(path_to_save)\n",
    "        except:\n",
    "            print('folder \"{}\" already exists'.format(path_to_save))\n",
    "        \n",
    "        self.num_texts = 0\n",
    "        for i, url in enumerate(self._traverse()):\n",
    "            print('({}) {}'.format(i, url))\n",
    "            try:\n",
    "                soup =  BeautifulSoup(urllib.request.urlopen(url))\n",
    "    \n",
    "                # kill all script and style elements\n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.extract()    # rip it out\n",
    "\n",
    "                # get text\n",
    "                text = soup.get_text()\n",
    "                \n",
    "                #replace eol, tabs, multiple whitespaces)\n",
    "                text = re.sub('\\\\t', ' ', text)\n",
    "                text = re.sub(' +', ' ', text)\n",
    "                text = re.sub(' \\n', '\\n', text)\n",
    "\n",
    "                #remove text in parentheses\n",
    "                text = re.sub(r'\\{[^}]*\\}', '', text)\n",
    "    \n",
    "                #remove html comments\n",
    "                #text = re.sub(re.compile(\"/\\*.*?\\*/\",re.DOTALL ), \"\", text)\n",
    "                #text = re.sub(re.compile(\"//.*?\\n\" ), \"\", text)\n",
    "                \n",
    "                #split to paragraphs of length >= 100\n",
    "                paragraphs = filter(lambda x: len(x) >= 100, \n",
    "                                    map(lambda x: x.strip(), text.split('\\n\\n')))\n",
    "                for i, p in enumerate(paragraphs):\n",
    "                    p = p.strip()\n",
    "                    #print('PARAGRAPH {}\\n[ {} ]\\n\\n'.format(i + 1, p))\n",
    "                    self._save_text(p, path_to_save=os.path.join(path_to_save, self._create_filename(url, i)))\n",
    "                    self.num_texts += 1\n",
    "            except:\n",
    "                print('invalid url: {}'.format(url))\n",
    "    \n",
    "    def iterate_over_texts(self, load_from='texts'):\n",
    "        print('iterate over {} saved texts'.format(self.num_texts))\n",
    "        for filename in sorted(filter(lambda x: not x.startswith('.'), os.listdir(load_from))):\n",
    "            with open(os.path.join(load_from, filename), 'rb') as file:\n",
    "                yield (filename, pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Base tokenizer/tokens classes and utilities.\"\"\"\n",
    "\n",
    "class Tokens(object):\n",
    "    \"\"\"A class to represent a list of tokenized text.\"\"\"\n",
    "    TEXT = 0\n",
    "    TEXT_WS = 1\n",
    "    SPAN = 2\n",
    "    POS = 3\n",
    "    LEMMA = 4\n",
    "    NER = 5\n",
    "\n",
    "    def __init__(self, data, annotators, opts=None):\n",
    "        self.data = data\n",
    "        self.annotators = annotators\n",
    "        self.opts = opts or {}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The number of tokens.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def slice(self, i=None, j=None):\n",
    "        \"\"\"Return a view of the list of tokens from [i, j).\"\"\"\n",
    "        new_tokens = copy.copy(self)\n",
    "        new_tokens.data = self.data[i: j]\n",
    "        return new_tokens\n",
    "\n",
    "    def untokenize(self):\n",
    "        \"\"\"Returns the original text (with whitespace reinserted).\"\"\"\n",
    "        return ''.join([t[self.TEXT_WS] for t in self.data]).strip()\n",
    "\n",
    "    def words(self, uncased=False):\n",
    "        \"\"\"Returns a list of the text of each token\n",
    "        Args:\n",
    "            uncased: lower cases text\n",
    "        \"\"\"\n",
    "        if uncased:\n",
    "            return [t[self.TEXT].lower() for t in self.data]\n",
    "        else:\n",
    "            return [t[self.TEXT] for t in self.data]\n",
    "\n",
    "    def offsets(self):\n",
    "        \"\"\"Returns a list of [start, end) character offsets of each token.\"\"\"\n",
    "        return [t[self.SPAN] for t in self.data]\n",
    "\n",
    "    def pos(self):\n",
    "        \"\"\"Returns a list of part-of-speech tags of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'pos' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.POS] for t in self.data]\n",
    "\n",
    "    def lemmas(self):\n",
    "        \"\"\"Returns a list of the lemmatized text of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'lemma' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.LEMMA] for t in self.data]\n",
    "\n",
    "    def entities(self):\n",
    "        \"\"\"Returns a list of named-entity-recognition tags of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'ner' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.NER] for t in self.data]\n",
    "\n",
    "    def ngrams(self, n=1, uncased=False, filter_fn=None, as_strings=True):\n",
    "        \"\"\"Returns a list of all ngrams from length 1 to n.\n",
    "        Args:\n",
    "            n: upper limit of ngram length\n",
    "            uncased: lower cases text\n",
    "            filter_fn: user function that takes in an ngram list and returns\n",
    "              True or False to keep or not keep the ngram\n",
    "            as_string: return the ngram as a string vs list\n",
    "        \"\"\"\n",
    "        def _skip(gram):\n",
    "            if not filter_fn:\n",
    "                return False\n",
    "            return filter_fn(gram)\n",
    "\n",
    "        words = self.words(uncased)\n",
    "        ngrams = [(s, e + 1)\n",
    "                  for s in range(len(words))\n",
    "                  for e in range(s, min(s + n, len(words)))\n",
    "                  if not _skip(words[s:e + 1])]\n",
    "\n",
    "        # Concatenate into strings\n",
    "        if as_strings:\n",
    "            ngrams = ['{}'.format(' '.join(words[s:e])) for (s, e) in ngrams]\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "    def entity_groups(self):\n",
    "        \"\"\"Group consecutive entity tokens with the same NER tag.\"\"\"\n",
    "        entities = self.entities()\n",
    "        if not entities:\n",
    "            return None\n",
    "        non_ent = self.opts.get('non_ent', '')\n",
    "        groups = []\n",
    "        idx = 0\n",
    "        while idx < len(entities):\n",
    "            ner_tag = entities[idx]\n",
    "            # Check for entity tag\n",
    "            if ner_tag != non_ent:\n",
    "                # Chomp the sequence\n",
    "                start = idx\n",
    "                while (idx < len(entities) and entities[idx] == ner_tag):\n",
    "                    idx += 1\n",
    "                groups.append((self.slice(start, idx).untokenize(), ner_tag))\n",
    "            else:\n",
    "                idx += 1\n",
    "        return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \"\"\"Base tokenizer class.\n",
    "    Tokenizers implement tokenize, which should return a Tokens class.\n",
    "    \"\"\"\n",
    "    def tokenize(self, text):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def shutdown(self):\n",
    "        pass\n",
    "\n",
    "    def __del__(self):\n",
    "        self.shutdown()\n",
    "\n",
    "\n",
    "class RuTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotators: set that can include pos, lemma, and ner.\n",
    "        \"\"\"\n",
    "        self.annotators = copy.deepcopy(kwargs.get('annotators', set()))\n",
    "        self.include_pos = {'pos'} & self.annotators\n",
    "        self.include_lemma = {'lemma'} & self.annotators\n",
    "        self.include_ner = {'ner'} & self.annotators\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.wt = WhitespaceTokenizer()\n",
    "        self.rt = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        #к нижнему регистру\n",
    "        text = text.lower()\n",
    "        \n",
    "        # We don't treat new lines as tokens.\n",
    "        clean_text = text.replace('\\n', ' ')\n",
    "        \n",
    "        # remove punctuation\n",
    "        clean_text = ' '.join(self.rt.tokenize(clean_text))\n",
    "        \n",
    "        # split by whitespaces and get spans\n",
    "        spans = list(self.wt.span_tokenize(clean_text))\n",
    "        n = len(spans)\n",
    "        \n",
    "        data = []\n",
    "        for i in range(n):\n",
    "            start_idx, end_idx = spans[i]\n",
    "            \n",
    "            token = clean_text[start_idx:end_idx]\n",
    "        \n",
    "            start_ws = start_idx  \n",
    "            if i + 1 < n:\n",
    "                end_ws = spans[i + 1][0]\n",
    "            else:\n",
    "                end_ws = start_idx + len(token)\n",
    "            \n",
    "            token_ws = clean_text[start_ws:end_ws]\n",
    "            \n",
    "            lemma, pos, ent_type = '', '', ''\n",
    "            if self.include_pos or self.include_lemma:\n",
    "                p = self.morph.parse(token)[0]\n",
    "                if self.include_lemma:\n",
    "                    lemma = p.normal_form\n",
    "                if self.include_pos:\n",
    "                    pos = p.tag.POS\n",
    "            \n",
    "            \n",
    "            if self.include_ner:\n",
    "                entities = Text(token, hint_language_code='ru').entities\n",
    "                if len(entities):\n",
    "                    ent_type = entities[0].tag\n",
    "            \n",
    "            data.append((token, token_ws, spans[i], pos, lemma, ent_type))\n",
    "\n",
    "        return Tokens(data, self.annotators, opts={'non_ent': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RuTokenizer(annotators={'lemma', 'pos', 'ner'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class QA:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.parser = HTMLParser(url)\n",
    "    \n",
    "    \n",
    "    def parse(self):\n",
    "        self.parser.parse()\n",
    "        \n",
    "\n",
    "    def calculate_overlap(self, p_tokens, q_tokens):\n",
    "        p_set = set(p_tokens.words())\n",
    "        q_set = set(q_tokens.words())\n",
    "        return len(p_set.intersection(q_set))\n",
    "\n",
    "    \n",
    "    def find_closest_paragraph(self, question):\n",
    "        question_tokens = tokenizer.tokenize(question)\n",
    "        max_overlap = None\n",
    "        max_match_paragraph = None\n",
    "        for filename, text in self.parser.iterate_over_texts():\n",
    "            paragraph_tokens = tokenizer.tokenize(text)\n",
    "            overlap = self.calculate_overlap(paragraph_tokens, question_tokens)\n",
    "            if max_overlap is None or overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                max_match_paragraph = text\n",
    "\n",
    "        return max_match_paragraph\n",
    "    \n",
    "    \n",
    "    def answer(self, question):\n",
    "        return self.find_closest_paragraph(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TfIdfQA:\n",
    "    def __init__(self, url):\n",
    "        self.parser = HTMLParser(url)\n",
    "        self.parser.parse()\n",
    "        self.texts = []\n",
    "        for filename, text in self.parser.iterate_over_texts():\n",
    "            self.texts.append(text.lower())\n",
    "            \n",
    "        self.tfIdf = TfidfVectorizer()\n",
    "        self.tokens = self.tfIdf.fit_transform(self.texts)\n",
    "        self.word_dict = self.tfIdf.vocabulary_\n",
    "        \n",
    "    def _build_one_hot_vector_by_string_(self, string):\n",
    "        description = np.zeros(len(self.word_dict))\n",
    "        for word in string.split():\n",
    "            if word in self.word_dict:\n",
    "                description[self.word_dict[word]] = 1\n",
    "        return description\n",
    "\n",
    "    def get_best_indx(self, question):\n",
    "        oh = self._build_one_hot_vector_by_string_(question)\n",
    "        scores = (self.tokens * oh)\n",
    "        #print((self.tokens * oh).shape)\n",
    "        #print(scores.shape)\n",
    "        \n",
    "        return np.argmax(scores)\n",
    "        #return len(p_set.intersection(q_set))\n",
    "\n",
    "    \n",
    "    def find_closest_paragraph(self, question):\n",
    "        return self.texts[self.get_best_indx(question)]\n",
    "    \n",
    "    def answer(self, question):\n",
    "        return self.find_closest_paragraph(question)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "URL = \"https://tutorial.djangogirls.org/ru/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize HTMLParser with \"https://tutorial.djangogirls.org/ru/\"\n"
     ]
    }
   ],
   "source": [
    "qa = QA(url=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder \"texts\" already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /usr/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) https://tutorial.djangogirls.org/ru/\n",
      "(1) https://tutorial.djangogirls.org/ru/installation/\n",
      "(2) https://tutorial.djangogirls.org/ru/how_the_internet_works/\n",
      "(3) https://tutorial.djangogirls.org/ru/intro_to_command_line/\n",
      "(4) https://tutorial.djangogirls.org/ru/python_installation/\n",
      "(5) https://tutorial.djangogirls.org/ru/code_editor/\n",
      "(6) https://tutorial.djangogirls.org/ru/python_introduction/\n",
      "(7) https://tutorial.djangogirls.org/ru/django/\n",
      "(8) https://tutorial.djangogirls.org/ru/django_installation/\n",
      "(9) https://tutorial.djangogirls.org/ru/django_start_project/\n",
      "(10) https://tutorial.djangogirls.org/ru/django_models/\n",
      "(11) https://tutorial.djangogirls.org/ru/django_admin/\n",
      "(12) https://tutorial.djangogirls.org/ru/deploy/\n",
      "(13) https://tutorial.djangogirls.org/ru/django_urls/\n",
      "(14) https://tutorial.djangogirls.org/ru/django_views/\n",
      "(15) https://tutorial.djangogirls.org/ru/html/\n",
      "(16) https://tutorial.djangogirls.org/ru/django_orm/\n",
      "(17) https://tutorial.djangogirls.org/ru/dynamic_data_in_templates/\n",
      "(18) https://tutorial.djangogirls.org/ru/django_templates/\n",
      "(19) https://tutorial.djangogirls.org/ru/css/\n",
      "(20) https://tutorial.djangogirls.org/ru/template_extending/\n",
      "(21) https://tutorial.djangogirls.org/ru/extend_your_application/\n",
      "(22) https://tutorial.djangogirls.org/ru/django_forms/\n",
      "(23) https://tutorial.djangogirls.org/ru/whats_next/\n",
      "(24) https://tutorial.djangogirls.org/ru/\n",
      "(25) https://tutorial.djangogirls.org/ru/how_the_internet_works/\n",
      "(26) https://tutorial.djangogirls.org/ru/installation/\n"
     ]
    }
   ],
   "source": [
    "qa.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize HTMLParser with \"https://tutorial.djangogirls.org/ru/\"\n",
      "folder \"texts\" already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /usr/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) https://tutorial.djangogirls.org/ru/\n",
      "(1) https://tutorial.djangogirls.org/ru/installation/\n",
      "(2) https://tutorial.djangogirls.org/ru/how_the_internet_works/\n",
      "(3) https://tutorial.djangogirls.org/ru/intro_to_command_line/\n",
      "(4) https://tutorial.djangogirls.org/ru/python_installation/\n",
      "(5) https://tutorial.djangogirls.org/ru/code_editor/\n",
      "(6) https://tutorial.djangogirls.org/ru/python_introduction/\n",
      "(7) https://tutorial.djangogirls.org/ru/django/\n",
      "(8) https://tutorial.djangogirls.org/ru/django_installation/\n",
      "(9) https://tutorial.djangogirls.org/ru/django_start_project/\n",
      "(10) https://tutorial.djangogirls.org/ru/django_models/\n",
      "(11) https://tutorial.djangogirls.org/ru/django_admin/\n",
      "(12) https://tutorial.djangogirls.org/ru/deploy/\n",
      "(13) https://tutorial.djangogirls.org/ru/django_urls/\n",
      "(14) https://tutorial.djangogirls.org/ru/django_views/\n",
      "(15) https://tutorial.djangogirls.org/ru/html/\n",
      "(16) https://tutorial.djangogirls.org/ru/django_orm/\n",
      "(17) https://tutorial.djangogirls.org/ru/dynamic_data_in_templates/\n",
      "(18) https://tutorial.djangogirls.org/ru/django_templates/\n",
      "(19) https://tutorial.djangogirls.org/ru/css/\n",
      "(20) https://tutorial.djangogirls.org/ru/template_extending/\n",
      "(21) https://tutorial.djangogirls.org/ru/extend_your_application/\n",
      "(22) https://tutorial.djangogirls.org/ru/django_forms/\n",
      "(23) https://tutorial.djangogirls.org/ru/whats_next/\n",
      "(24) https://tutorial.djangogirls.org/ru/\n",
      "(25) https://tutorial.djangogirls.org/ru/how_the_internet_works/\n",
      "(26) https://tutorial.djangogirls.org/ru/installation/\n",
      "iterate over 447 saved texts\n"
     ]
    }
   ],
   "source": [
    "tfIdfQa = TfIdfQA(url=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "question = 'Как добавить определения блоков для селекторов?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterate over 447 saved texts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Теперь добавим определения блоков для различных селекторов. Селекторы, которые начинают с символа ., относятся к классам. В Интернете много хороших справочников по CSS, которые могут помочь тебе понять следующий код. А сейчас просто скопируй и вставь код в файл djangogirls/static/css/blog.css:\\nblog/static/css/blog.css\\n.page-header'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мы хотим использовать это фрагмент в твоём шаблоне для отображения содержимого. пора добавить теги блоков в этот файл!\\nнам нужно, чтобы новый тег блока соответствовал тегу в файле base.html. также нам необходимо включить весь код, который соответствует твоему блоку с содержимым. для этого расположи всё между  и . вот так:\\nblog/templates/blog/post_list.html'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfQa.answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
